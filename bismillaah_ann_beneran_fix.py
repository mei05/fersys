# -*- coding: utf-8 -*-
"""Bismillaah ANN BENERAN FIX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18eFIDbkKUiBtIwL-sqG2TnJo2fn9Rg67
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

import math
from keras.models import Sequential
from keras.layers import Dense
# from keras.layers import LSTM
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

import itertools
import warnings
warnings.filterwarnings('ignore')
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)

plt.style.use('fivethirtyeight')#drawing style
window=12#Time Window
amount_of_features=60#number of features
d=0.05#dropout coefficient/modulus
batch_size=60#Number of training batches
epoch = 100 #450 #470 #total iterations
output = 12
filename = 'main_ch.csv'
STORAGE_FOLDER = 'FOLDER'
save_file='predict_result_ann.csv'

rainfall_data_matrix = pd.read_csv(filename)
rainfall_data_matrix.drop(rainfall_data_matrix.filter(regex="Unnamed"),axis=1, inplace=True)
rainfall_data_matrix.set_index('THN', inplace=True)
rainfall_data_matrix = rainfall_data_matrix.transpose()
rainfall_data_matrix

dates = pd.date_range(start='2008-01', freq='MS', periods=len(rainfall_data_matrix.columns)*12)
dates

rainfall_data_matrix = rainfall_data_matrix.transpose()
rainfall_data_matrix_np = rainfall_data_matrix.interpolate()
rainfall_data_matrix_np

jan_median = rainfall_data_matrix_np["JAN"].median()
rainfall_data_matrix_np['JAN'] = rainfall_data_matrix_np["JAN"].fillna(jan_median)
feb_median = rainfall_data_matrix_np["FEB"].median()
rainfall_data_matrix_np['FEB'] = rainfall_data_matrix_np["FEB"].fillna(feb_median)
mar_median = rainfall_data_matrix_np["MAR"].median()
rainfall_data_matrix_np['MAR'] = rainfall_data_matrix_np["MAR"].fillna(mar_median)
apr_median = rainfall_data_matrix_np["APR"].median()
rainfall_data_matrix_np['APR'] = rainfall_data_matrix_np["APR"].fillna(apr_median)
mei_median = rainfall_data_matrix_np["MEI"].median()
rainfall_data_matrix_np['MEI'] = rainfall_data_matrix_np["MEI"].fillna(mei_median)
jun_median = rainfall_data_matrix_np["JUN"].median()
rainfall_data_matrix_np['JUN'] = rainfall_data_matrix_np["JUN"].fillna(jun_median)
jul_median = rainfall_data_matrix_np["JUN"].median()
rainfall_data_matrix_np['JUL'] = rainfall_data_matrix_np["JUL"].fillna(jul_median)
agt_median = rainfall_data_matrix_np["AGT"].median()
rainfall_data_matrix_np['AGT'] = rainfall_data_matrix_np["AGT"].fillna(agt_median)
sep_median = rainfall_data_matrix_np["SEP"].median()
rainfall_data_matrix_np['SEP'] = rainfall_data_matrix_np["SEP"].fillna(sep_median)
okt_median = rainfall_data_matrix_np["OKT"].median()
rainfall_data_matrix_np['OKT'] = rainfall_data_matrix_np["OKT"].fillna(okt_median)
nov_median = rainfall_data_matrix_np["NOV"].median()
rainfall_data_matrix_np['NOV'] = rainfall_data_matrix_np["NOV"].fillna(nov_median)
des_median = rainfall_data_matrix_np["DES"].median()
rainfall_data_matrix_np['DES'] = rainfall_data_matrix_np["DES"].fillna(des_median)
rainfall_data_matrix_np

plt.figure(figsize=(15,8))
colors=['lightcoral', 'yellow', 'lime', 'slategray', 'pink', 'magenta', 'red', 'green', 'blue', 'cyan', 'black',
        'orange', 'navy', 'purple', 'brown']
plt.gca().set_prop_cycle(color=colors)
plt.plot(rainfall_data_matrix_np.transpose(), linewidth=2.25)
plt.xlabel('Bulan')
plt.ylabel('Curah Hujan(mm)')
plt.title('Curah Hujan Bulanan Setiap Tahun')
plt.legend(labels=['2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019', '2020', '2021', '2022'],
           ncol=2, labelspacing=0.05)

rainfall_data_matrix_np = rainfall_data_matrix_np.to_numpy()
from sklearn.model_selection import train_test_split

shape = rainfall_data_matrix_np.shape
rainfall_data_matrix_np = rainfall_data_matrix_np.reshape((shape[0] * shape[1], 1))
rainfall_data = pd.DataFrame({'Precipitation': rainfall_data_matrix_np[:,0]})
rainfall_data.set_index(dates, inplace=True)

rainfall_data = rainfall_data.copy()

# apply normalization techniques
for column in rainfall_data.columns:
    min_data = rainfall_data[column].min()
    max_data = rainfall_data[column].max()
    rainfall_data[column] = (rainfall_data[column] - rainfall_data[column].min()) / (rainfall_data[column].max() - rainfall_data[column].min())

test_rainfall_data = rainfall_data.iloc[-24:]
rainfall_data = rainfall_data.iloc[:-24]

rainfall_data

#Which column of the data set to choose
data=rainfall_data['Precipitation']

#Construct a dataset for a machine learning model
data=data.values
dataset=data

for i in range(output-1):
    zero=np.zeros(i+1)
    temp=np.append(data[i+1:],zero)
    dataset=np.row_stack((dataset,temp))
dataset=pd.DataFrame(dataset).T
dataset=dataset.iloc[:-(window+window)]
dataset

#Divide Features and Labels
X=dataset.iloc[:,:window]
y=dataset.iloc[:,-output:]
X,y=X.values,y.values
# print(X)

X.shape

y.shape

#Build ANN model training
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Dropout
#Build the training model process
model = Sequential()#Build a hierarchical model
model.add(Dense(360,input_dim=window,activation='relu'))#Build the LSTM layer
model.add(Dropout(d))
model.add(Dense(output))
model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])
model.summary()

#Neural Network Training and Results
history = model.fit(X, y, epochs =epoch, batch_size = batch_size,verbose=1,validation_split=0.25, shuffle= False) #Training model nb_epoch times

# data test (1 tahun terakhir)
test=test_rainfall_data
test=np.reshape(test.values[-12:], (1, window))
#actual forecast testing data (input for next prediction)
predict=model.predict(test)[0,:]
predict

predict=np.reshape(predict[:], (1, window))
mse = mean_squared_error(test,  predict)
print(mse)

from sklearn.metrics import mean_absolute_percentage_error
mape = mean_absolute_percentage_error(test,  predict)
print(mape*100)

mae = mean_absolute_error(test,  predict)
print(mae)

rmse = math.sqrt(mse)
print(rmse)

predict = predict * (max_data - min_data) + min_data
predict = predict.astype(int)
print(predict)

# result = np.array(result)
tahun = 2023
dicthn = {"THN": tahun}
key = ['JAN','FEB','MAR', 'APR', 'MEI', 'JUN', 'JUL', 'AGT', 'SEP', 'OKT', 'NOV', 'DES']
dic = dict(zip(key, predict.flatten()))
dicthn.update(dic)

# dic = dict(enumerate(result.flatten(), 1))
print(dicthn)

#iterate over images
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(epoch)
plt.plot(epochs_range, loss, label='Train Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Train and Val Loss')
plt.show()

#iterate over images
loss = history.history['accuracy']
val_loss = history.history['val_accuracy']
epochs_range = range(epoch)
plt.plot(epochs_range, loss, label='Train Accuracy')
plt.plot(epochs_range, val_loss, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Train and Val Accuracy')
plt.show()

data_times=np.arange(len(data))
all_data=np.hstack((data,result))
predicted_times=np.arange(len(all_data))
plt.figure(figsize=(15, 5))
plt.axvline(len(data), linestyle="dotted", linewidth=5, color='r')
predicted_lines = plt.plot(predicted_times, all_data, label="prediction", color="b")
data_lines = plt.plot(data_times, data, label="data", color="k")
plt.legend(handles=[data_lines[0], predicted_lines[0]],loc="upper left")
#Create separate legends for training, validation, and prediction data
plt.show()

import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)